{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from geofeather.pygeos import to_geofeather, from_geofeather\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "#import pygeos\n",
    "from rasterstats import zonal_stats\n",
    "from scipy.stats import spearmanr\n",
    "import shapely\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "from shapely.geometry import mapping, shape\n",
    "from shapely import wkb\n",
    "from shapely.wkb import loads as from_wkb\n",
    "\n",
    "import rasterio\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.windows import Window\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "import fiona\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import reproject\n",
    "from rasterio.enums import Resampling as ResamplingEnums\n",
    "from rasterio.features import rasterize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4ef57",
   "metadata": {},
   "source": [
    "# Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91129ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiling(input_path, tile_size, overlap, pad_value):\n",
    "    \"\"\"\n",
    "    Generator that yields raster tiles in-memory, with padding.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to input GeoTIFF.\n",
    "        tile_size (tuple): (width, height) of each tile in pixels.\n",
    "        overlap (tuple): (x_overlap, y_overlap) in pixels.\n",
    "        pad_value (int or float): Value used to pad edge tiles.\n",
    "\n",
    "    Yields:\n",
    "        dict: {\n",
    "            \"data\": np.ndarray (bands, height, width),\n",
    "            \"transform\": Affine transform of the tile,\n",
    "            \"indices\": (top, left) pixel coordinates in source\n",
    "        }\n",
    "    \"\"\"\n",
    "    tile_width, tile_height = tile_size\n",
    "    overlap_x, overlap_y = overlap\n",
    "\n",
    "    with rasterio.open(input_path) as src:\n",
    "        width, height = src.width, src.height\n",
    "        num_bands = src.count\n",
    "        step_x = tile_width - overlap_x\n",
    "        step_y = tile_height - overlap_y\n",
    "\n",
    "        for top in range(0, height, step_y):\n",
    "            for left in range(0, width, step_x):\n",
    "                win_width = min(tile_width, width - left)\n",
    "                win_height = min(tile_height, height - top)\n",
    "                window = Window(left, top, win_width, win_height)\n",
    "                transform = src.window_transform(window)\n",
    "\n",
    "                data = src.read(window=window)\n",
    "\n",
    "                if win_width < tile_width or win_height < tile_height:\n",
    "                    padded = np.full((num_bands, tile_height, tile_width), pad_value, dtype=data.dtype)\n",
    "                    padded[:, :win_height, :win_width] = data\n",
    "                    data = padded\n",
    "\n",
    "                yield {\n",
    "                    \"data\": data,\n",
    "                    \"transform\": transform,\n",
    "                    \"indices\": (top, left)\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa76a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object tiling at 0x000002D801E5A5A0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GDP tiles\n",
    "gdp_dates_2030 = (\"GDP2030_025_ssp1_clipped\", \"GDP2030_025_ssp2_clipped\", \"GDP2030_025_ssp3_clipped\", \"GDP2030_025_ssp4_clipped\", \"GDP2030_025_ssp5_clipped\")\n",
    "gdp_dates_2050 = (\"GDP2050_025_ssp1_clipped\", \"GDP2050_025_ssp2_clipped\", \"GDP2050_025_ssp3_clipped\", \"GDP2050_025_ssp4_clipped\", \"GDP2050_025_ssp5_clipped\")\n",
    "gdp_dates_2100 = (\"GDP2100_025_ssp1_clipped\", \"GDP2100_025_ssp2_clipped\", \"GDP2100_025_ssp3_clipped\", \"GDP2100_025_ssp4_clipped\", \"GDP2100_025_ssp5_clipped\")\n",
    "\n",
    "\n",
    "tiling(\n",
    "    input_path=\"GDP_clipped_files_025d\\GDP2030__025_ssp1_clipped.tif\",\n",
    "    tile_size=(64, 64),\n",
    "    overlap=(0,0), \n",
    "    pad_value=0\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233926c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c82f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce3c0fc6",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff8b46",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815058f",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InfrastructureDataset(Dataset):\n",
    "    def __init__(self, input_raster_path, label_raster_path,\n",
    "                 tile_size=(64, 64), overlap=(0, 0), pad_value=0, transform=None):\n",
    "        self.input_raster_path = input_raster_path\n",
    "        self.label_raster_path = label_raster_path\n",
    "        self.tile_size = tile_size\n",
    "        self.overlap = overlap\n",
    "        self.pad_value = pad_value\n",
    "        self.transform = transform\n",
    "\n",
    "        # Precompute tile indices\n",
    "        self.tile_indices = []\n",
    "        with rasterio.open(self.input_raster_path) as src:\n",
    "            self.width, self.height = src.width, src.height\n",
    "            self.num_bands = src.count\n",
    "            step_x = tile_size[0] - overlap[0]\n",
    "            step_y = tile_size[1] - overlap[1]\n",
    "\n",
    "            for top in range(0, self.height, step_y):\n",
    "                for left in range(0, self.width, step_x):\n",
    "                    self.tile_indices.append((top, left))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tile_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        top, left = self.tile_indices[idx]\n",
    "        tile_width, tile_height = self.tile_size\n",
    "\n",
    "        # Read input tile\n",
    "        with rasterio.open(self.input_raster_path) as src:\n",
    "            window = Window(left, top, tile_width, tile_height)\n",
    "            input_tile = src.read(window=window)\n",
    "\n",
    "            if input_tile.shape[1] < tile_height or input_tile.shape[2] < tile_width:\n",
    "                padded = np.full((self.num_bands, tile_height, tile_width), self.pad_value, dtype=input_tile.dtype)\n",
    "                padded[:, :input_tile.shape[1], :input_tile.shape[2]] = input_tile\n",
    "                input_tile = padded\n",
    "\n",
    "        # Read label tile (single-band)\n",
    "        with rasterio.open(self.label_raster_path) as lbl_src:\n",
    "            label_tile = lbl_src.read(1, window=window)\n",
    "\n",
    "            if label_tile.shape[0] < tile_height or label_tile.shape[1] < tile_width:\n",
    "                padded_label = np.full((tile_height, tile_width), self.pad_value, dtype=label_tile.dtype)\n",
    "                padded_label[:label_tile.shape[0], :label_tile.shape[1]] = label_tile\n",
    "                label_tile = padded_label\n",
    "\n",
    "        if self.transform:\n",
    "            input_tile, label_tile = self.transform(input_tile, label_tile)\n",
    "\n",
    "        return torch.tensor(input_tile, dtype=torch.float32), torch.tensor(label_tile, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9f88b96",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "dataset = InfrastructureDataset(\n",
    "    input_raster_path=\"cisi_index_pop_all_years.tif\",       # input: predictor\n",
    "    label_raster_path=\"CISI_label_file_025.tif\",          # label: infrastructure presence in 2020\n",
    "    tile_size=(64, 64),\n",
    "    overlap=(0, 0),\n",
    "    pad_value=0\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c67948c",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # downsample\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, out_channels, 1)  # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)  # upsample\n",
    "        x = self.decoder(x)\n",
    "        return x  # Output: [B, out_channels, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5d9f5",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Training setup\n",
    "# select model\n",
    "model = SimpleCNN(in_channels=1, out_channels=1)  # Adjust in_channels if you have more bands\n",
    "\n",
    "# move model to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function (regression)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# define optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81852d76",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, loader, criterion, optimizer, num_epochs=10, device=\"cpu\"):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device).unsqueeze(1).float()  # [B, 1, H, W]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss / len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a7c3fb",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: inf\n",
      "Epoch 2/20 - Loss: inf\n",
      "Epoch 3/20 - Loss: inf\n",
      "Epoch 4/20 - Loss: inf\n",
      "Epoch 5/20 - Loss: inf\n",
      "Epoch 6/20 - Loss: inf\n",
      "Epoch 7/20 - Loss: inf\n",
      "Epoch 8/20 - Loss: inf\n",
      "Epoch 9/20 - Loss: inf\n",
      "Epoch 10/20 - Loss: inf\n",
      "Epoch 11/20 - Loss: inf\n",
      "Epoch 12/20 - Loss: inf\n",
      "Epoch 13/20 - Loss: inf\n",
      "Epoch 14/20 - Loss: inf\n",
      "Epoch 15/20 - Loss: inf\n",
      "Epoch 16/20 - Loss: inf\n",
      "Epoch 17/20 - Loss: inf\n",
      "Epoch 18/20 - Loss: inf\n",
      "Epoch 19/20 - Loss: inf\n",
      "Epoch 20/20 - Loss: inf\n"
     ]
    }
   ],
   "source": [
    "# Run the training model\n",
    "train(model, loader, criterion, optimizer, num_epochs=20, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a6e5640",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input min/max: 0.0 0.7352721095085144\n",
      "Label min/max: -9223372036854775808 0\n"
     ]
    }
   ],
   "source": [
    "for x, y in loader:\n",
    "    print(\"Input min/max:\", x.min().item(), x.max().item())\n",
    "    print(\"Label min/max:\", y.min().item(), y.max().item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79b1b5",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac9f9d",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TorchGeo + U-Net (SMP) for continuous per-pixel regression (pixelwise regression)\n",
    "- Inputs: multi-band georasters (e.g., pop, GDP, land-use)\n",
    "- Labels: single-band continuous target in [0,1] (e.g., CISI), float32\n",
    "- Pairing: IntersectionDataset (by spatial overlap)\n",
    "- Loss: masked Huber (handles NoData via NaN)\n",
    "- Mixed precision + grad clipping\n",
    "\n",
    "Install:\n",
    "    pip install torch torchvision torchgeo rasterio segmentation-models-pytorch albumentations\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -----------------------------\n",
    "# TorchGeo imports (version-robust)\n",
    "# -----------------------------\n",
    "from torchgeo.datasets import RasterDataset\n",
    "\n",
    "# stack_samples import moved across versions; try new then old\n",
    "try:\n",
    "    from torchgeo.datasets import stack_samples  # newer\n",
    "except Exception:\n",
    "    try:\n",
    "        from torchgeo.datasets.utils import stack_samples  # older\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Could not import stack_samples from TorchGeo.\") from e\n",
    "\n",
    "# IntersectionDataset location varies slightly across versions\n",
    "try:\n",
    "    from torchgeo.datasets.geo import IntersectionDataset\n",
    "except Exception:\n",
    "    try:\n",
    "        from torchgeo.datasets import IntersectionDataset\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Could not import IntersectionDataset from TorchGeo.\") from e\n",
    "\n",
    "from torchgeo.samplers import GridGeoSampler, RandomGeoSampler\n",
    "\n",
    "# -----------------------------\n",
    "# Model (U-Net from SMP)\n",
    "# -----------------------------\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset definitions (TorchGeo)\n",
    "# -----------------------------\n",
    "class InputsDataset(RasterDataset):\n",
    "    \"\"\"\n",
    "    Multi-band inputs (e.g., population, GDP, land-use) as aligned GeoTIFFs.\n",
    "    \"\"\"\n",
    "    filename_glob = \"*.tif\"\n",
    "    is_image = True  # tensors will be under key \"image\"\n",
    "\n",
    "\n",
    "class LabelsDataset(RasterDataset):\n",
    "    \"\"\"\n",
    "    Single-band continuous target (e.g., CISI scaled to [0,1]).\n",
    "    IMPORTANT:\n",
    "      - use float32 for continuous targets\n",
    "      - set is_image=False so TorchGeo stores this under key \"mask\"\n",
    "      - store NoData as NaN in your GeoTIFFs when possible\n",
    "    \"\"\"\n",
    "    filename_glob = \"*.tif\"\n",
    "    is_image = False          # tensors will be under key \"mask\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Normalization module\n",
    "# -----------------------------\n",
    "class ChannelWiseNormalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        m = torch.as_tensor(mean).view(1, -1, 1, 1)\n",
    "        s = torch.as_tensor(std).view(1, -1, 1, 1)\n",
    "        self.register_buffer(\"mean\", m.float())\n",
    "        self.register_buffer(\"std\", s.float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Masked Huber regression loss\n",
    "# -----------------------------\n",
    "class MaskedHuber(nn.Module):\n",
    "    def __init__(self, delta=0.5):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, pred, target, mask=None):\n",
    "        \"\"\"\n",
    "        pred, target: (B,1,H,W)\n",
    "        mask: (B,1,H,W) boolean/float in {0,1}. If None, uses isfinite(target).\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.isfinite(target)\n",
    "        mask = mask.float()\n",
    "\n",
    "        diff = pred - target\n",
    "        abs_diff = diff.abs()\n",
    "        delta = torch.tensor(self.delta, device=pred.device)\n",
    "        quadratic = torch.minimum(abs_diff, delta)\n",
    "        loss = 0.5 * quadratic**2 + delta * (abs_diff - quadratic) - 0.5 * (delta**2)\n",
    "        loss = loss * mask\n",
    "\n",
    "        denom = mask.sum().clamp_min(1.0)\n",
    "        return loss.sum() / denom\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training / validation driver\n",
    "# -----------------------------\n",
    "def run(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ---- Build base datasets (positional root arg)\n",
    "    inputs_ds = InputsDataset(args.in_dir)\n",
    "    labels_ds = LabelsDataset(args.lab_dir)\n",
    "\n",
    "    # ---- Combine by spatial overlap (modern replacement for ZipDataset)\n",
    "    dataset = IntersectionDataset(inputs_ds, labels_ds)   # equivalently: inputs_ds & labels_ds\n",
    "\n",
    "    # ---- Samplers\n",
    "    train_sampler = RandomGeoSampler(dataset, size=args.patch, length=args.train_windows)\n",
    "    val_sampler = GridGeoSampler(dataset, size=args.patch, stride=int(args.patch * args.val_stride_frac))\n",
    "\n",
    "    # ---- DataLoaders (use TorchGeo collate)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=args.batch, sampler=train_sampler, num_workers=args.workers,\n",
    "        collate_fn=stack_samples, pin_memory=True, persistent_workers=args.workers > 0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset, batch_size=args.batch, sampler=val_sampler, num_workers=args.workers,\n",
    "        collate_fn=stack_samples, pin_memory=True, persistent_workers=args.workers > 0\n",
    "    )\n",
    "\n",
    "    # ---- Peek for channel count and sanity\n",
    "    sample = next(iter(train_loader))\n",
    "    x0 = sample[0][\"image\"]      # (B,C,H,W)\n",
    "    y0 = sample[1][\"mask\"]       # (B,1,H,W)\n",
    "    in_ch = x0.shape[1]\n",
    "    print(f\"[Info] Inferred input channels: {in_ch}\")\n",
    "    print(f\"[Info] Label shape sample: {tuple(y0.shape)} (expect B,1,H,W)\")\n",
    "\n",
    "    # ---- Normalization\n",
    "    if args.use_quick_norm:\n",
    "        mean, std = quick_channel_stats(train_loader, in_ch, args.norm_batches)\n",
    "    else:\n",
    "        mean = [0.0] * in_ch\n",
    "        std = [1.0] * in_ch\n",
    "    print(f\"[Info] Normalization mean: {mean}\")\n",
    "    print(f\"[Info] Normalization std : {std}\")\n",
    "\n",
    "    normalize = ChannelWiseNormalize(mean, std).to(device)\n",
    "\n",
    "    # ---- Model: SMP U-Net\n",
    "    model = smp.Unet(\n",
    "        encoder_name=args.backbone,     # e.g., \"resnet34\"\n",
    "        encoder_weights=None,           # use \"imagenet\" only if inputs are truly RGB-like\n",
    "        in_channels=in_ch,\n",
    "        classes=1,                      # single continuous target channel\n",
    "        activation=None,                # handle activation explicitly\n",
    "    ).to(device)\n",
    "\n",
    "    # ---- Optimizer, loss, AMP\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    criterion = MaskedHuber(delta=args.huber_delta)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n",
    "\n",
    "    # ---- Training loop\n",
    "    best_val = math.inf\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        tr_loss, tr_n = 0.0, 0\n",
    "        for batch in train_loader:\n",
    "            x = batch[0][\"image\"].float().to(device)  # (B,C,H,W)\n",
    "            y = batch[1][\"mask\"].float().to(device)   # (B,1,H,W) in [0,1]; NaN = NoData preferred\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=args.amp):\n",
    "                x = normalize(x)\n",
    "                yhat = model(x)\n",
    "                yhat = torch.sigmoid(yhat)  # constrain to [0,1]\n",
    "                loss = criterion(yhat, y)   # masked inside (NaNs ignored)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_n += 1\n",
    "\n",
    "        # ---- Validation\n",
    "        model.eval()\n",
    "        va_loss, va_n = 0.0, 0\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=args.amp):\n",
    "            for batch in val_loader:\n",
    "                x = batch[0][\"image\"].float().to(device)\n",
    "                y = batch[1][\"mask\"].float().to(device)\n",
    "                x = normalize(x)\n",
    "                yhat = torch.sigmoid(model(x))\n",
    "                loss = criterion(yhat, y)\n",
    "                va_loss += loss.item()\n",
    "                va_n += 1\n",
    "\n",
    "        tr_loss /= max(tr_n, 1)\n",
    "        va_loss /= max(va_n, 1)\n",
    "        print(f\"Epoch {epoch:03d} | train: {tr_loss:.4f} | val: {va_loss:.4f}\")\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            ckpt = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"norm_mean\": mean,\n",
    "                \"norm_std\": std,\n",
    "                \"in_channels\": in_ch,\n",
    "                \"backbone\": args.backbone,\n",
    "            }\n",
    "            Path(args.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(ckpt, Path(args.out_dir) / \"best_unet_regression.pt\")\n",
    "            print(f\"  -> saved best checkpoint (val={best_val:.4f})\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def quick_channel_stats(train_loader, in_ch, norm_batches):\n",
    "    \"\"\"Rough channel-wise mean/std over a few batches.\"\"\"\n",
    "    running_mean = torch.zeros(in_ch, dtype=torch.float64)\n",
    "    running_var = torch.zeros(in_ch, dtype=torch.float64)\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            xb = batch[0][\"image\"].float()  # (B,C,H,W)\n",
    "            B, C, H, W = xb.shape\n",
    "            xb = xb.view(B, C, -1)\n",
    "            running_mean += xb.mean(dim=(0, 2)).double()\n",
    "            running_var += xb.var(dim=(0, 2), unbiased=False).double()\n",
    "            n += 1\n",
    "            if i + 1 >= norm_batches:\n",
    "                break\n",
    "    mean = (running_mean / max(n, 1)).float().tolist()\n",
    "    std = (running_var / max(n, 1)).sqrt().float().tolist()\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI\n",
    "# -----------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--in_dir\", type=str, required=True, help=\"Path to inputs directory (multi-band GeoTIFFs)\")\n",
    "    p.add_argument(\"--lab_dir\", type=str, required=True, help=\"Path to labels directory (single-band GeoTIFFs)\")\n",
    "    p.add_argument(\"--out_dir\", type=str, default=\"checkpoints\", help=\"Where to save checkpoints\")\n",
    "\n",
    "    p.add_argument(\"--patch\", type=int, default=256, help=\"Patch size in pixels (H=W)\")\n",
    "    p.add_argument(\"--train_windows\", type=int, default=2000, help=\"#random windows per epoch for training\")\n",
    "    p.add_argument(\"--val_stride_frac\", type=float, default=1.0, help=\"Val stride as fraction of patch size (>=1)\")\n",
    "    p.add_argument(\"--batch\", type=int, default=4)\n",
    "    p.add_argument(\"--workers\", type=int, default=4)\n",
    "\n",
    "    p.add_argument(\"--epochs\", type=int, default=50)\n",
    "    p.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    p.add_argument(\"--wd\", type=float, default=1e-2)\n",
    "    p.add_argument(\"--huber_delta\", type=float, default=0.5)\n",
    "    p.add_argument(\"--backbone\", type=str, default=\"resnet34\")  # any SMP encoder\n",
    "    p.add_argument(\"--amp\", action=\"store_true\", help=\"Enable mixed precision\")\n",
    "\n",
    "    p.add_argument(\"--use_quick_norm\", action=\"store_true\", help=\"Estimate mean/std over a few batches\")\n",
    "    p.add_argument(\"--norm_batches\", type=int, default=10, help=\"#batches for quick norm estimation\")\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class Args: pass\n",
    "    args = Args()\n",
    "    args.in_dir = \"data/inputs\"\n",
    "    args.lab_dir = \"data/labels\"\n",
    "    args.out_dir = \"checkpoints\"\n",
    "\n",
    "    args.patch = 256\n",
    "    args.train_windows = 500\n",
    "    args.val_stride_frac = 1.0\n",
    "    args.batch = 4\n",
    "    args.workers = 2\n",
    "\n",
    "    args.epochs = 5\n",
    "    args.lr = 3e-4\n",
    "    args.wd = 1e-2\n",
    "    args.huber_delta = 0.5\n",
    "    args.backbone = \"resnet34\"\n",
    "    args.amp = True\n",
    "\n",
    "    args.use_quick_norm = True\n",
    "    args.norm_batches = 5\n",
    "\n",
    "    run(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
