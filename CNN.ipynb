{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6488715b",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset not found in `paths='data/inputs'` and cannot be automatically downloaded, either specify a different `paths` or manually download the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 312\u001b[0m\n\u001b[0;32m    309\u001b[0m args\u001b[38;5;241m.\u001b[39muse_quick_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    310\u001b[0m args\u001b[38;5;241m.\u001b[39mnorm_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m--> 312\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 130\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    127\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# ---- Build base datasets (positional root arg)\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m inputs_ds \u001b[38;5;241m=\u001b[39m \u001b[43mInputsDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m labels_ds \u001b[38;5;241m=\u001b[39m LabelsDataset(args\u001b[38;5;241m.\u001b[39mlab_dir)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# ---- Combine by spatial overlap (modern replacement for ZipDataset)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchgeo\\datasets\\geo.py:499\u001b[0m, in \u001b[0;36mRasterDataset.__init__\u001b[1;34m(self, paths, crs, res, bands, transforms, cache)\u001b[0m\n\u001b[0;32m    496\u001b[0m             i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseparate_files:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mband_indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mDatasetNotFoundError\u001b[0m: Dataset not found in `paths='data/inputs'` and cannot be automatically downloaded, either specify a different `paths` or manually download the dataset."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TorchGeo + U-Net (SMP) for continuous per-pixel regression (pixelwise regression)\n",
    "- Inputs: multi-band georasters (e.g., pop, GDP, land-use)\n",
    "- Labels: single-band continuous target in [0,1] (e.g., CISI), float32\n",
    "- Pairing: IntersectionDataset (by spatial overlap)\n",
    "- Loss: masked Huber (handles NoData via NaN)\n",
    "- Mixed precision + grad clipping\n",
    "\n",
    "Install:\n",
    "    pip install torch torchvision torchgeo rasterio segmentation-models-pytorch albumentations\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -----------------------------\n",
    "# TorchGeo imports (version-robust)\n",
    "# -----------------------------\n",
    "from torchgeo.datasets import RasterDataset\n",
    "\n",
    "# stack_samples import moved across versions; try new then old\n",
    "try:\n",
    "    from torchgeo.datasets import stack_samples  # newer\n",
    "except Exception:\n",
    "    try:\n",
    "        from torchgeo.datasets.utils import stack_samples  # older\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Could not import stack_samples from TorchGeo.\") from e\n",
    "\n",
    "# IntersectionDataset location varies slightly across versions\n",
    "try:\n",
    "    from torchgeo.datasets.geo import IntersectionDataset\n",
    "except Exception:\n",
    "    try:\n",
    "        from torchgeo.datasets import IntersectionDataset\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"Could not import IntersectionDataset from TorchGeo.\") from e\n",
    "\n",
    "from torchgeo.samplers import GridGeoSampler, RandomGeoSampler\n",
    "\n",
    "# -----------------------------\n",
    "# Model (U-Net from SMP)\n",
    "# -----------------------------\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset definitions (TorchGeo)\n",
    "# -----------------------------\n",
    "class InputsDataset(RasterDataset):\n",
    "    \"\"\"\n",
    "    Multi-band inputs (e.g., population, GDP, land-use) as aligned GeoTIFFs.\n",
    "    \"\"\"\n",
    "    filename_glob = \"*.tif\"\n",
    "    is_image = True  # tensors will be under key \"image\"\n",
    "\n",
    "\n",
    "class LabelsDataset(RasterDataset):\n",
    "    \"\"\"\n",
    "    Single-band continuous target (e.g., CISI scaled to [0,1]).\n",
    "    IMPORTANT:\n",
    "      - use float32 for continuous targets\n",
    "      - set is_image=False so TorchGeo stores this under key \"mask\"\n",
    "      - store NoData as NaN in your GeoTIFFs when possible\n",
    "    \"\"\"\n",
    "    filename_glob = \"*.tif\"\n",
    "    is_image = False          # tensors will be under key \"mask\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Normalization module\n",
    "# -----------------------------\n",
    "class ChannelWiseNormalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        m = torch.as_tensor(mean).view(1, -1, 1, 1)\n",
    "        s = torch.as_tensor(std).view(1, -1, 1, 1)\n",
    "        self.register_buffer(\"mean\", m.float())\n",
    "        self.register_buffer(\"std\", s.float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Masked Huber regression loss\n",
    "# -----------------------------\n",
    "class MaskedHuber(nn.Module):\n",
    "    def __init__(self, delta=0.5):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, pred, target, mask=None):\n",
    "        \"\"\"\n",
    "        pred, target: (B,1,H,W)\n",
    "        mask: (B,1,H,W) boolean/float in {0,1}. If None, uses isfinite(target).\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.isfinite(target)\n",
    "        mask = mask.float()\n",
    "\n",
    "        diff = pred - target\n",
    "        abs_diff = diff.abs()\n",
    "        delta = torch.tensor(self.delta, device=pred.device)\n",
    "        quadratic = torch.minimum(abs_diff, delta)\n",
    "        loss = 0.5 * quadratic**2 + delta * (abs_diff - quadratic) - 0.5 * (delta**2)\n",
    "        loss = loss * mask\n",
    "\n",
    "        denom = mask.sum().clamp_min(1.0)\n",
    "        return loss.sum() / denom\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training / validation driver\n",
    "# -----------------------------\n",
    "def run(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ---- Build base datasets (positional root arg)\n",
    "    inputs_ds = InputsDataset(args.in_dir)\n",
    "    labels_ds = LabelsDataset(args.lab_dir)\n",
    "\n",
    "    # ---- Combine by spatial overlap (modern replacement for ZipDataset)\n",
    "    dataset = IntersectionDataset(inputs_ds, labels_ds)   # equivalently: inputs_ds & labels_ds\n",
    "\n",
    "    # ---- Samplers\n",
    "    train_sampler = RandomGeoSampler(dataset, size=args.patch, length=args.train_windows)\n",
    "    val_sampler = GridGeoSampler(dataset, size=args.patch, stride=int(args.patch * args.val_stride_frac))\n",
    "\n",
    "    # ---- DataLoaders (use TorchGeo collate)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=args.batch, sampler=train_sampler, num_workers=args.workers,\n",
    "        collate_fn=stack_samples, pin_memory=True, persistent_workers=args.workers > 0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset, batch_size=args.batch, sampler=val_sampler, num_workers=args.workers,\n",
    "        collate_fn=stack_samples, pin_memory=True, persistent_workers=args.workers > 0\n",
    "    )\n",
    "\n",
    "    # ---- Peek for channel count and sanity\n",
    "    sample = next(iter(train_loader))\n",
    "    x0 = sample[0][\"image\"]      # (B,C,H,W)\n",
    "    y0 = sample[1][\"mask\"]       # (B,1,H,W)\n",
    "    in_ch = x0.shape[1]\n",
    "    print(f\"[Info] Inferred input channels: {in_ch}\")\n",
    "    print(f\"[Info] Label shape sample: {tuple(y0.shape)} (expect B,1,H,W)\")\n",
    "\n",
    "    # ---- Normalization\n",
    "    if args.use_quick_norm:\n",
    "        mean, std = quick_channel_stats(train_loader, in_ch, args.norm_batches)\n",
    "    else:\n",
    "        mean = [0.0] * in_ch\n",
    "        std = [1.0] * in_ch\n",
    "    print(f\"[Info] Normalization mean: {mean}\")\n",
    "    print(f\"[Info] Normalization std : {std}\")\n",
    "\n",
    "    normalize = ChannelWiseNormalize(mean, std).to(device)\n",
    "\n",
    "    # ---- Model: SMP U-Net\n",
    "    model = smp.Unet(\n",
    "        encoder_name=args.backbone,     # e.g., \"resnet34\"\n",
    "        encoder_weights=None,           # use \"imagenet\" only if inputs are truly RGB-like\n",
    "        in_channels=in_ch,\n",
    "        classes=1,                      # single continuous target channel\n",
    "        activation=None,                # handle activation explicitly\n",
    "    ).to(device)\n",
    "\n",
    "    # ---- Optimizer, loss, AMP\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    criterion = MaskedHuber(delta=args.huber_delta)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n",
    "\n",
    "    # ---- Training loop\n",
    "    best_val = math.inf\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        tr_loss, tr_n = 0.0, 0\n",
    "        for batch in train_loader:\n",
    "            x = batch[0][\"image\"].float().to(device)  # (B,C,H,W)\n",
    "            y = batch[1][\"mask\"].float().to(device)   # (B,1,H,W) in [0,1]; NaN = NoData preferred\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=args.amp):\n",
    "                x = normalize(x)\n",
    "                yhat = model(x)\n",
    "                yhat = torch.sigmoid(yhat)  # constrain to [0,1]\n",
    "                loss = criterion(yhat, y)   # masked inside (NaNs ignored)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_n += 1\n",
    "\n",
    "        # ---- Validation\n",
    "        model.eval()\n",
    "        va_loss, va_n = 0.0, 0\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=args.amp):\n",
    "            for batch in val_loader:\n",
    "                x = batch[0][\"image\"].float().to(device)\n",
    "                y = batch[1][\"mask\"].float().to(device)\n",
    "                x = normalize(x)\n",
    "                yhat = torch.sigmoid(model(x))\n",
    "                loss = criterion(yhat, y)\n",
    "                va_loss += loss.item()\n",
    "                va_n += 1\n",
    "\n",
    "        tr_loss /= max(tr_n, 1)\n",
    "        va_loss /= max(va_n, 1)\n",
    "        print(f\"Epoch {epoch:03d} | train: {tr_loss:.4f} | val: {va_loss:.4f}\")\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            ckpt = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"norm_mean\": mean,\n",
    "                \"norm_std\": std,\n",
    "                \"in_channels\": in_ch,\n",
    "                \"backbone\": args.backbone,\n",
    "            }\n",
    "            Path(args.out_dir).mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(ckpt, Path(args.out_dir) / \"best_unet_regression.pt\")\n",
    "            print(f\"  -> saved best checkpoint (val={best_val:.4f})\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def quick_channel_stats(train_loader, in_ch, norm_batches):\n",
    "    \"\"\"Rough channel-wise mean/std over a few batches.\"\"\"\n",
    "    running_mean = torch.zeros(in_ch, dtype=torch.float64)\n",
    "    running_var = torch.zeros(in_ch, dtype=torch.float64)\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            xb = batch[0][\"image\"].float()  # (B,C,H,W)\n",
    "            B, C, H, W = xb.shape\n",
    "            xb = xb.view(B, C, -1)\n",
    "            running_mean += xb.mean(dim=(0, 2)).double()\n",
    "            running_var += xb.var(dim=(0, 2), unbiased=False).double()\n",
    "            n += 1\n",
    "            if i + 1 >= norm_batches:\n",
    "                break\n",
    "    mean = (running_mean / max(n, 1)).float().tolist()\n",
    "    std = (running_var / max(n, 1)).sqrt().float().tolist()\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI\n",
    "# -----------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--in_dir\", type=str, required=True, help=\"Path to inputs directory (multi-band GeoTIFFs)\")\n",
    "    p.add_argument(\"--lab_dir\", type=str, required=True, help=\"Path to labels directory (single-band GeoTIFFs)\")\n",
    "    p.add_argument(\"--out_dir\", type=str, default=\"checkpoints\", help=\"Where to save checkpoints\")\n",
    "\n",
    "    p.add_argument(\"--patch\", type=int, default=256, help=\"Patch size in pixels (H=W)\")\n",
    "    p.add_argument(\"--train_windows\", type=int, default=2000, help=\"#random windows per epoch for training\")\n",
    "    p.add_argument(\"--val_stride_frac\", type=float, default=1.0, help=\"Val stride as fraction of patch size (>=1)\")\n",
    "    p.add_argument(\"--batch\", type=int, default=4)\n",
    "    p.add_argument(\"--workers\", type=int, default=4)\n",
    "\n",
    "    p.add_argument(\"--epochs\", type=int, default=50)\n",
    "    p.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    p.add_argument(\"--wd\", type=float, default=1e-2)\n",
    "    p.add_argument(\"--huber_delta\", type=float, default=0.5)\n",
    "    p.add_argument(\"--backbone\", type=str, default=\"resnet34\")  # any SMP encoder\n",
    "    p.add_argument(\"--amp\", action=\"store_true\", help=\"Enable mixed precision\")\n",
    "\n",
    "    p.add_argument(\"--use_quick_norm\", action=\"store_true\", help=\"Estimate mean/std over a few batches\")\n",
    "    p.add_argument(\"--norm_batches\", type=int, default=10, help=\"#batches for quick norm estimation\")\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class Args: pass\n",
    "    args = Args()\n",
    "    args.in_dir = \"data/inputs\"\n",
    "    args.lab_dir = \"data/labels\"\n",
    "    args.out_dir = \"checkpoints\"\n",
    "\n",
    "    args.patch = 256\n",
    "    args.train_windows = 500\n",
    "    args.val_stride_frac = 1.0\n",
    "    args.batch = 4\n",
    "    args.workers = 2\n",
    "\n",
    "    args.epochs = 5\n",
    "    args.lr = 3e-4\n",
    "    args.wd = 1e-2\n",
    "    args.huber_delta = 0.5\n",
    "    args.backbone = \"resnet34\"\n",
    "    args.amp = True\n",
    "\n",
    "    args.use_quick_norm = True\n",
    "    args.norm_batches = 5\n",
    "\n",
    "    run(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c117c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs exists?  False\n",
      "labels exists?  False\n",
      "# *.tif in inputs: 0\n",
      "# **/*.tif in inputs: 0\n",
      "# *.tiff in inputs: 0\n",
      "# **/*.tiff in inputs: 0\n",
      "# **/*.tif in labels: 0\n",
      "# **/*.tiff in labels: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "print(\"inputs exists? \", Path(\"data/inputs\").exists())\n",
    "print(\"labels exists? \", Path(\"data/labels\").exists())\n",
    "print(\"# *.tif in inputs:\", len(glob(\"data/inputs/*.tif\")))\n",
    "print(\"# **/*.tif in inputs:\", len(glob(\"data/inputs/**/*.tif\", recursive=True)))\n",
    "print(\"# *.tiff in inputs:\", len(glob(\"data/inputs/*.tiff\")))\n",
    "print(\"# **/*.tiff in inputs:\", len(glob(\"data/inputs/**/*.tiff\", recursive=True)))\n",
    "\n",
    "print(\"# **/*.tif in labels:\", len(glob(\"data/labels/**/*.tif\", recursive=True)))\n",
    "print(\"# **/*.tiff in labels:\", len(glob(\"data/labels/**/*.tiff\", recursive=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8eaa2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
